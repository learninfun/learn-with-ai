1. Word Embedding Model是一種自然語言處理的技術，用來將文字轉換為向量形式，以便進行機器學習和人工智慧等任務。

2. 常見的Word Embedding Model包括：CBOW、Skip-gram、GloVe等。

3. CBOW模型又稱為Continuous Bag-of-Words模型，是基於單詞上下文預測中心詞的模型。

4. Skip-gram模型則是基於中心詞預測上下文詞的模型。

5. GloVe是基於全局詞頻統計的方法，用來獲得單詞之間的相對關係。

6. Word Embedding Model的訓練需要大量的語料庫數據，並且需要適當的調整參數才能獲得較好的結果。

7. Word Embedding Model能夠提高自然語言處理的效率和準確性，廣泛應用於文本分類、情感分析、機器翻譯等領域。