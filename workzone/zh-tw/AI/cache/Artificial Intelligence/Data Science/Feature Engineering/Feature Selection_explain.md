Feature Selection（特徵選擇）是在機器學習中的一個重要步驟，可用來挑選對模型訓練最有幫助的特徵（features），以提高模型的準確度、降低過度擬合（overfitting）的風險，同時也能夠減少模型的訓練時間和複雜度。

例如，假設我們有一個房價預測的問題，資料包含了很多不同的特徵，像是房子的面積、房間的數量、地理位置等。但在這些特徵當中，有些可能並不是對於預測房價有很大的影響力，甚至可能是噪音（noise）或冗餘（redundant）的特徵。因此，透過Feature Selection的方法，我們可以挑選出對於預測房價有較大貢獻的特徵，例如只選擇房子的面積或房間數量等等，並且忽略其他不必要的特徵，來訓練一個更簡潔、更好的模型。

在實務上，Feature Selection的方法有很多種，例如「Filter methods」、「Wrapper methods」、「Embedded methods」等等，透過這些方法可以根據資料的特性，選擇最適合的方法進行特徵選擇，以提高模型的準確度和效能。