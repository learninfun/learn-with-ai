

1. What are the commonly used asymptotic notations in algorithm analysis, and how do they differ in terms of growth rate?
2. How do you determine the time complexity of an algorithm using asymptotic notation?
3. What is the significance of the big O notation, and how does it help in analyzing algorithms?
4. Can you provide examples of algorithms with different growth rates and their corresponding asymptotic notations?
5. In what situations would you prefer using asymptotic notation over other methods of algorithm analysis, such as benchmark testing?