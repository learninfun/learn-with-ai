

1. Divide and Conquer: Breaking down a problem into smaller sub-problems that are easier to solve, and then combining their solutions to solve the original problem.

2. Dynamic Programming: Finding the optimal solution by breaking down a problem into smaller sub-problems, then systematically solving them and storing the solutions in memory to avoid re-computation.

3. Greedy Algorithms: Making the locally optimal choice at each step, hoping that the choice will ultimately lead to the global solution.

4. Backtracking: Systematically exploring all possible solutions to a problem by incrementally building candidates, and backtracking when a solution is found to be invalid.

5. Branch and Bound: Similar to Backtracking, but using heuristics to identify partial solutions that are likely to lead to an optimal solution, and then exploring only those possibilities.

6. Randomized Algorithms: Using randomization to introduce some unpredictability into the search for a solution, often leading to faster and more efficient algorithms.

7. Approximation Algorithms: Finding a near-optimal solution, rather than an exact one, by making some assumptions or approximations about the problem.

8. Heuristic Algorithms: Using a "rule-of-thumb" or "intuition" to quickly find a good solution, without necessarily being able to guarantee that it is the optimal one.