1. Batch Normalization is a technique used to improve the training rates of deep neural networks.


2. Batch Normalization works by normalizing the input layer by subtracting the batch mean and dividing by the batch standard deviation.


3. Batch Normalization can also be used on non-input layers to normalize the activations.


4. Batch Normalization helps to reduce the internal covariate shift, which is the phenomenon where the distribution of input activations changes during training, and slows down learning.


5. Batch Normalization makes the optimization function more well-behaved, leading to faster convergence and improved generalization.


6. Batch Normalization introduces two additional hyperparameters, epsilon and beta, which need to be chosen carefully to achieve optimal performance.


7. Although Batch Normalization is primarily used with convolutional neural networks, it can also be effectively used with other types of neural networks.