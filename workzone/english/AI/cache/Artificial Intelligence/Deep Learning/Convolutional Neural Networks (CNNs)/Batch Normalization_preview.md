1. What is Batch Normalization and how does it work to improve neural network training?
2. What are the benefits and limitations of Batch Normalization in deep learning models?
3. How does Batch Normalization affect the gradient propagation during backpropagation?
4. Under what circumstances could Batch Normalization be detrimental to model performance?
5. How does the Batch Normalization layer differ from other normalization techniques (such as Layer Normalization or Instance Normalization)?