A loss function is a measure of how different the predicted output of a machine learning model is from the actual output. In other words, it's a mathematical function that quantifies the error between the predicted output and the expected output.

For example, in linear regression, the loss function is commonly defined as the mean squared error (MSE) between the actual values and predicted values. The MSE is calculated by taking the average of the squared differences between the predicted and actual values:

MSE = 1/n * Σ(yi - ŷi)² 

where n is the number of data points, yi is the actual output, and ŷi is the predicted output.

Other examples of loss functions include cross-entropy loss, hinge loss, and cosine similarity loss. The choice of loss function depends on the specific machine learning problem and the type of output being predicted.