1. What is the basic structure of a Gated Recurrent Unit (GRU)? 
2. How does a GRU differ from other types of recurrent neural networks (RNNs)? 
3. What are the key benefits of using a GRU in natural language processing (NLP) tasks? 
4. How are the gates in a GRU model trained to improve its performance? 
5. Can a GRU be used as a generative model for text or speech synthesis? If so, how?