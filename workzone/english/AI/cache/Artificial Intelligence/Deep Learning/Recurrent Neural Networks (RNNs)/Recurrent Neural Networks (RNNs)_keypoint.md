1. RNNs are a type of neural network designed to process sequential data, such as time-series or language data.

2. Unlike feedforward neural networks, RNNs have connections between the neurons that form a cycle, which allows them to keep track of previous input values and produce output based on this context.

3. RNNs use a variant of backpropagation called backpropagation through time (BPTT) to update their weights and biases during training.

4. RNNs can be used for a wide variety of tasks, including natural language processing, image captioning, speech recognition, and stock price prediction.

5. One challenge with RNNs is dealing with the problem of vanishing and exploding gradients, which can occur when the gradients become too small or too large as they are propagated back through time.

6. To overcome this problem, several types of RNN models have been developed, including LSTM (long short-term memory) and GRU (gated recurrent unit) networks, which include additional mechanisms to control the flow of information through the network. 

7. RNNs can be stacked to create deep RNN architectures that are capable of processing more complex data sequences.

8. RNNs are capable of producing both sequences (such as in language modeling) and single-point predictions (such as in classification tasks).