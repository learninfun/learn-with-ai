1. Definition: Feature engineering refers to the process of creating new features or variables from raw data that are more informative and relevant for machine learning models.

2. Importance: Feature engineering is critical for improving model performance by making the data more understandable, simpler to process, and reducing noise.

3. Data Preprocessing: The first step in feature engineering is data preprocessing, which involves cleaning the data, handling missing values, and scaling and normalizing the data.

4. Feature Selection: The next step is feature selection, which involves identifying the most relevant and significant features from the dataset.

5. Feature Extraction: Feature extraction involves deriving new features from existing ones through mathematical or statistical techniques such as PCA, Discrete Fourier Transform or Wavelet Transform. 

6. Domain Knowledge: Domain knowledge in such an important factor when it comes to feature engineering. Often, a deep understanding of the problem domain is required to identify and engineer meaningful features.

7. Feature Types: There are several common types of features, such as numerical, categorical, binary, text, and image features. Each of these types requires different methods of feature engineering. 

8. Feature Representation: Features can be represented in different forms such as vectors, matrices or tensors depending on the type of data.

9. Validation: Finally, it is important to validate the engineered features on test data to ensure that the model is not overfitting and the performance is consistent. 

10. Iterative Process: Feature engineering is an iterative process, which may require several rounds of experimentation, validation, and feedback before arriving at the optimal set of features.