Feature selection is a technique in machine learning and data mining that focuses on selecting a subset of relevant features (variables, predictors) for the model while discarding the rest. This is done to improve model performance, reduce the risk of overfitting, and increase the interpretability of the model.

Example: Suppose we want to predict housing prices based on a dataset containing 100 features such as age of the building, distance to the nearest school, number of bathrooms, etc. It may not be necessary or useful to use all the features as some may be redundant, unimportant or even noisy. We can use feature selection techniques to identify the most relevant features that contribute the most to the prediction task. For example, we can use the correlation between each feature and the target variable (housing prices) to select only the top 10 most correlated features. These 10 features could then be used to train a model that is simpler, more efficient, and more accurate than using all 100 features.