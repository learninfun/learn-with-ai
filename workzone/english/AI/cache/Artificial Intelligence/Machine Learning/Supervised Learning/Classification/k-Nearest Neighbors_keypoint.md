1. The k-Nearest Neighbors (k-NN) algorithm is a non-parametric and lazy supervised learning algorithm which is used for classification and regression tasks.
2. It works by finding the k closest data points to a new observation in the feature space and then assigning the label of the most common class among those k points as the prediction for the new observation.
3. The distance metric used to calculate distances between data points is usually Euclidean distance or Manhattan distance.
4. The choice of k has a significant impact on the model's performance, and it is typically chosen by cross-validation or other empirical methods.
5. The k-NN algorithm has high computational complexity when dealing with large datasets, and its performance can be improved with techniques such as dimensionality reduction, data normalization, and distance weighting.
6. The k-NN algorithm can suffer from the curse of dimensionality, as the distance between data points becomes less meaningful in high-dimensional spaces.
7. The k-NN algorithm is sensitive to noisy data and outliers, which can negatively affect its performance.