Q-Learning is a type of reinforcement learning algorithm that is used to learn and make decisions in an environment with unknown dynamics. In this algorithm, an agent interacts with an environment and receives rewards based on its action. The agent uses these rewards to learn the optimal action to take in a particular state of the environment, by updating a function called a Q-value. The Q-value represents the expected cumulative reward that the agent will receive if it takes a particular action in a particular state, and follows a certain policy.

An example of Q-Learning is teaching an agent to play a game of tic-tac-toe. The objective of the game is to place three marks in a row, column, or diagonal, before the opponent does. The agent would start with no prior knowledge of the game, and would learn as it plays against itself. In each state of the game, the agent would have several possible actions to take, such as placing a mark or choosing a previously taken spot. The agent would receive a reward of +1 if it wins, -1 if it loses, and 0 for a draw. The Q-values would be updated after each move, with the agent choosing the action with the highest Q-value for a given state. With enough training, the agent would learn the optimal strategy for winning the game.