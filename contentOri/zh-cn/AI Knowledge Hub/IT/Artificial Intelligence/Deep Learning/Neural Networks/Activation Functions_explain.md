Activation functions是神经网路中的一个重要元素，它决定了每个神经元的输出值。Activation functions可以将输入的信号转换成不同的形式，在深度学习领域中常常使用以下的几种Activation functions：

1. Sigmoid Function：将输入的值经由sigmoid函数转换，输出范围在0到1之间。Sigmoid函数在binary classification中的应用很广泛。

2. Tanh Function：tanh函数跟sigmoid函数比较相似，但输出值范围在-1到1之间，以及加速神经元的收敛速度。

3. ReLU Function：ReLU函数是近年来深度学习中很受欢迎的Activation functions之一，它在输入大于0的情况下直接输出，输入小于0的时候则输出0。ReLU函数可以加速神经元的收敛速度。

4. Softmax Function：用于多类别判断的神经网路中。Softmax函数将输入的多个值转换成概率分布，让神经网路可以对多个类别进行分类。

以上是一些常见的Activation functions，不同的网络模型可能会选择不同的Activation functions，根据任务的不同有时可以使用不同的Activation functions 组合。