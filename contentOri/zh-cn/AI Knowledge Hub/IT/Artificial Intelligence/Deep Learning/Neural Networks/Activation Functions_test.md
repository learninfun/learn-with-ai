1. ReLU(Rectified Linear Unit)是一種常用的Activation Function，請問在什麼情況下會使得輸出為0？
答案：當輸入小於0時，ReLU會輸出0。

2. Sigmoid是一種Activation Function，它可以將輸入轉化為0到1之間的輸出值，請問當輸入是正無限大或負無限大時，Sigmoid的輸出值為多少？
答案：當輸入是正無限大時，Sigmoid的輸出值接近於1；當輸入是負無限大時，Sigmoid的輸出值接近於0。

3. Tanh是一種常用的Activation Function，它可以將輸入轉化為-1到1之間的輸出值，請問當輸入是正無限大或負無限大時，Tanh的輸出值為多少？
答案：當輸入是正無限大時，Tanh的輸出值接近於1；當輸入是負無限大時，Tanh的輸出值接近於-1。

4. LeakyReLU是一種Activation Function，它與ReLU相似但在輸入小於0時會保持一個較小的斜率，請問LeakyReLU的斜率通常為多少？
答案：LeakyReLU的斜率通常為0.01。

5. Softmax是一種Activation Function，它通常用於多分類問題，請問Softmax的輸出值是什麼意思？
答案：Softmax的輸出值表示每個分類的概率，所有分類的概率之和等於1。