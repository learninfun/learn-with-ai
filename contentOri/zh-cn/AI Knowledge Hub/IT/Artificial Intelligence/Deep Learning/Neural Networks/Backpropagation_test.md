1. 在神经网路中，如何计算误差梯度？

答案：误差梯度可以使用反向传播演算法来计算。首先从输出层开始，计算输出层的误差，然后将误差反向传播到输入层，逐层计算每一层的误差梯度。

2. 什么是局部最小值问题？怎样避免出现局部最小值问题？

答案：局部最小值问题是神经网路优化中常见的问题，当神经网路在训练过程中收敛到一个局部最小值时，就无法再继续学习。为了避免出现局部最小值问题，可以使用随机初始权重、增加训练数据、使用正则化等方法。

3. 为什么要使用激活函数？它的作用是什么？

答案：激活函数是神经元的非线性转换，主要作用是引入非线性因素，提高神经网路的表示能力。在没有激活函数的情况下，神经网路只能进行线性转换，无法处理复杂的非线性问题。

4. 什么是批量梯度下降法？它与Mini-batch梯度下降法有什么不同？

答案：批量梯度下降法是指每次计算梯度时将所有训练数据都纳入考虑，然后根据求得的梯度来更新权重。而Mini-batch梯度下降法是指每次计算梯度时只考虑部分训练数据，然后根据求得的梯度来更新权重。相对而言，Mini-batch梯度下降法可以降低计算成本，在实际应用中更常用。

5. 在神经网路训练中，过拟合问题如何解决？

答案：过拟合问题可以通过正则化等方法来解决。正则化的主要作用是限制权重的大小，防止神经网路过度依赖单个训练数据或噪声。在实际应用中，可以使用L1正则化、L2正则化等不同的正则化方法。