Feature Selection（特征选择）是在机器学习中的一个重要步骤，可用来挑选对模型训练最有帮助的特征（features），以提高模型的准确度、降低过度拟合（overfitting）的风险，同时也能够减少模型的训练时间和复杂度。

例如，假设我们有一个房价预测的问题，资料包含了很多不同的特征，像是房子的面积、房间的数量、地理位置等。但在这些特征当中，有些可能并不是对于预测房价有很大的影响力，甚至可能是噪音（noise）或冗余（redundant）的特征。因此，透过Feature Selection的方法，我们可以挑选出对于预测房价有较大贡献的特征，例如只选择房子的面积或房间数量等等，并且忽略其他不必要的特征，来训练一个更简洁、更好的模型。

在实务上，Feature Selection的方法有很多种，例如“Filter methods”、“Wrapper methods”、“Embedded methods”等等，透过这些方法可以根据资料的特性，选择最适合的方法进行特征选择，以提高模型的准确度和效能。