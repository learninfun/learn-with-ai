1. Word Embedding Model是一种自然语言处理的技术，用来将文字转换为向量形式，以便进行机器学习和人工智慧等任务。

2. 常见的Word Embedding Model包括：CBOW、Skip-gram、GloVe等。

3. CBOW模型又称为Continuous Bag-of-Words模型，是基于单词上下文预测中心词的模型。

4. Skip-gram模型则是基于中心词预测上下文词的模型。

5. GloVe是基于全局词频统计的方法，用来获得单词之间的相对关系。

6. Word Embedding Model的训练需要大量的语料库数据，并且需要适当的调整参数才能获得较好的结果。

7. Word Embedding Model能够提高自然语言处理的效率和准确性，广泛应用于文本分类、情感分析、机器翻译等领域。