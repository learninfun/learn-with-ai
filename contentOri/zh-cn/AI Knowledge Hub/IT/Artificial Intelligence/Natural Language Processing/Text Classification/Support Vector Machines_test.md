1. Support Vector Machines (SVM) 在什么情况下表现最佳？
2. 请解释SVM中的Kernel Trick是怎么运作的？
3. 如何处理在SVM中的类别不平衡问题？
4. 当SVM中的两类资料点完全分开，且具有较大的边缘(margin)时，该如何进行超平面的选择？
5. SVM的目标函数是什么，并请说明其所代表的意义？

答案：
1. SVM在到达最佳分类效能时，且拥有良好的数据分类能力。
2. Kernel Trick 以非线性转换将资料映射至高维度空间，藉此实现在低维度空间中难以分割的资料，因而提高分类的效能。
3. 常见的处理方式有：使用代价数据，将代价赋予错误预测的情况；使用在不同特征上的重量，以排除资料不平衡的影响。
4. 超平面的选择会影响SVM的分类效能，因此可透过"soft margin"的方法，允许资料点在小部分情况下不被分开。
5. SVM的目标函数为最小化平方加权的姐妹距离，其所代表的意义为：以最大差距为标准，将支援向量分布在相反方向，以产生最大化的margin(边界)。