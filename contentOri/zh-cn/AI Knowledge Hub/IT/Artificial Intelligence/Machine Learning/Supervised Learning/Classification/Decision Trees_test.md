1. 有一個資料集包含了1000筆資料，每筆資料有4個特徵及1個二元標籤（1或0）。訓練一個二元Decision Tree來預測標籤。如果使用全部資料訓練，Decision Tree的深度最多可以設為多少？
答案：log2(1000) = 9.97，所以Decision Tree的深度應該設為10。

2. 在一個Decision Tree中，某個節點的Gini impurity值為0.3，該節點有5個樣本，其中2個屬於正例，3個屬於負例。如果把該節點分裂為兩個子節點，如何最小化Gini impurity？
答案：計算左右兩個子節點的加權Gini impurity，找到最小值即可。例如，將兩個樣本分配到左子節點，另外三個樣本分配到右子節點，得到左子節點的Gini impurity值為0，右子節點的Gini impurity值為0.444。因此，最好的分裂方式是將兩個樣本分配到左子節點，另外三個樣本分配到右子節點。

3. 在一個Decision Tree中，某個節點的Gini impurity值為0.4，該節點有3個樣本，其中2個屬於正例，1個屬於負例。考慮一個二元特徵x，將節點分裂為左右兩個子節點，如果x等於1則進入右子節點，否則進入左子節點。如果x等分界值t則分裂為左右兩個子節點。請問，如何計算x等分界值t？
答案：從小到大將特徵x的值排序，用每個值當作分界值t，計算Gini impurity的下降量，找到最大的下降量對應的分界值t即可。例如，排序後的特徵值序列為(0, 0, 1)，各自對應的標籤值為(0, 1, 1)，計算t為0.5的下降量為0.067，計算t為1的下降量為0.25，因此最好的分界值是1。

4. 有一個Decision Tree訓練完畢後，其中某個節點A分裂為兩個子節點B和C，如果將子節點B進一步分裂，則Gini impurity值下降為0.1，如果將子節點C進一步分裂，則Gini impurity值下降為0.05。請問，是否可以將子節點C進一步分裂？
答案：不能確定，因為沒有考慮到整棵Decision Tree的訓練過程。如果在子節點C之前已經有其他節點分裂了，則子節點C進一步分裂可能會導致過度擬合，並且沒有明顯的收益。如果尚未有其他節點分裂，則可以考慮將子節點C進一步分裂，以提高整個Decision Tree的泛化能力。

5. 在一個Decision Tree中，某個節點的Gini impurity值為0.4，該節點有3個樣本，其中2個屬於正例，1個屬於負例。考慮一個多元特徵(x1,x2)，將節點分裂為左右兩個子節點，如果x1等於1且x2等於0則進入左子節點，否則進入右子節點。如果x1等分界值t1，x2等分界值t2則分裂為左右兩個子節點。請問，如何計算x1和x2等分界值t1和t2？
答案：計算每對(x1,x2)的Gini impurity下降量，找到最大的下降量對應的分界值t1和t2即可。例如，將(x1,x2)分為4個區域，各自對應的標籤值為(0, 0, 1, 1)，計算左區域的Gini impurity值為0，右區域的Gini impurity值為0.5。接著，計算x1等於0.5和x2等於0.5的下降量為0.167，計算x1等於0.5和x2等於1的下降量為0.3，計算x1等於1和x2等於0.5的下降量為0.167，計算x1等於1和x2等於1的下降量為0，因此最好的分界值是x1等於0.5，x2等於0.5。